{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute before using this notebook if using google colab\n",
    "\n",
    "kernel = str(get_ipython())\n",
    "\n",
    "if 'google.colab' in kernel:    \n",
    "    !wget https://raw.githubusercontent.com/fredzett/rmqa/master/utils.py -P local_modules -nc \n",
    "    !npx degit fredzett/rmqa/data data\n",
    "    import sys\n",
    "    sys.path.append('local_modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from utils import Datasets, plot_density, plot_hist, plot_line\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why estimate uncertainty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics / data analysis we are commonly estimating **parameters** of the population, i.e. we are estimating numerical characteristics such as the mean or the standard deviation, e.g. \n",
    "\n",
    "- what percentage of US voters will vote for Joe Biden?\n",
    "\n",
    "- how satisfied are our customers with our service?\n",
    "\n",
    "- what is the relation between body weight and exercise time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, most of the time it is not feasible and/or to costly to determine these parameters directly from the population which is why we commonly deal with (representative) samples. We then estimate the parameter in question from our sample and treat it as an\n",
    "**estimator** for the true population mean. \n",
    "\n",
    "- a sample mean is then an estimator for the population mean \n",
    "\n",
    "- a sample standard deviation is then an estimator for the popoulation standard deviation\n",
    "\n",
    "- a sample proportion is then an estimator for the population proportion\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "While it is easy to make such estimate it is crucial to understand the uncertainty and the possible margin of error of this estimate. \n",
    "\n",
    "There exist numerous examples (e.g. in journalism) where uncertainty around an estimator is not acknowledged and it is common to only state the point estimate (or summary statistic) instead of the plausible range in which the true parameter may lie. A good example is the news about unemployment figures. Many people don't realize that these numbers commonly are an estimate based on a sample. However, in stead of stating the range the point estimate is always communicated thereby treated as if certain. \n",
    "\n",
    "**In research we cannot be that slopy and need to understand and determine the uncertainty associated with our estimators.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to estimate uncertainty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have calculated a point estimate (e.g. the mean of a sample). How sure can we be that this estimate actually is close to the (true) mean of the population? After all, we don't have data about the enitre population, which is why we are **estimating** a statistic from a sample, instead of **calculating** a parameter from a population. \n",
    "\n",
    "Example:   \n",
    "Let's assume we conduct a randomized controlled trial to test the effectivenss of a new drug. Our trial yields that the average outcome differs between the treamtment and the control groups. \n",
    "\n",
    "How do we know that this difference is large enough to state that the groups are actually different, i.e. that the drug is actually effective?\n",
    "\n",
    "- Could this be due to chance?\n",
    "\n",
    "- Is the difference large enough that chance cannot explain the difference?\n",
    "\n",
    "- How large does the difference have to be that chance cannot explain the difference?\n",
    "\n",
    "\n",
    "We cannot assume that the point estimator actually matches the true parameter which is why we should calculate a range of values and a probability that the true parameter is within this range. \n",
    "\n",
    "There are two approaches to achieve this:\n",
    "\n",
    "1. using classical statistical concepts and probability theory\n",
    "\n",
    "2. using simulation and intuition\n",
    "\n",
    "In the following we will cover both approaches. Note that the first approach is usually covered in introductory statistics courses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of uncertainty using probability theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From undergraduate statistics you may recall that a sampling distribution is the distribituion of a random sample based estimate (e.g. the mean or the standard deviation). It is the distribution of estimates when taking $n$ different samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "We will use (fake) population data on online spending for all family households in Little Town. The data contains:\n",
    "\n",
    "a. amount of online spending\n",
    "\n",
    "b. indicator if family has children (1 = yes, 0 = no)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Datasets.fake_spent_children()\n",
    "np.mean(data[:,0]), np.mean(data[:,1]), data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we are not in possession of this large data set of the enitre population and instead we only have a sample of $n=50$. We are interested in two estimators:\n",
    "\n",
    "a. mean ($\\bar{x}$) online spending\n",
    "\n",
    "b. proportion ($p$) of families that has children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below both parameters are estimated using a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_sample(data, size):\n",
    "    total_rows = len(data)\n",
    "    rows_smpl = np.random.choice(range(total_rows), size=size, replace=False)\n",
    "    return data[rows_smpl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_size = 50\n",
    "smpl = take_sample(data, smpl_size)\n",
    "xbar, p = np.mean(smpl, axis=0)\n",
    "xbar, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat this for $1.000$ times to create a sampling distribution. The histograms below give an approximation of the sampling distributions of both estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = 1000\n",
    "estimates = np.empty((sims,2))\n",
    "for i in range(sims):\n",
    "    smpl = take_sample(data, smpl_size)\n",
    "    estimates[i] = np.mean(smpl, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = estimates[:,0]\n",
    "ps = estimates[:,1]\n",
    "np.mean(means), np.mean(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density(means, title=\"Density of sample mean spending \\nfrom $1.000$ random samples of size $50$ each\", xlabel=r\"Values for $\\bar{x}$\", ylabel=\"Probability density\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density(ps, title=\"Density of sample proportion of children \\nfrom $1.000$ random samples of size $50$ each\", xlabel=r\"Values for $p$\", ylabel=\"Probability density\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, in practice we usually only have one sample (e.g. our sample of $5.000$ households). However, the above procedure illustrates that \n",
    "\n",
    "- in practice many different samples are possible\n",
    "\n",
    "- each sample therefore generates a different estimator (as proxy for the true parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the above sampling distributions in more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected value of $\\bar{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Th sampling distribution of $\\bar(X)$ is the probability distribution of all possible values of the sample mean (in our case online spent). Given we are commonly only in possesssion of one sample we are interested in the mean of all possible values, the **expected value**. \n",
    "\n",
    "The **expected value of $\\bar{X}$** is given by:\n",
    "\n",
    "$$E(\\bar{X}) = \\mu$$\n",
    "\n",
    "$\\text{where}$  \n",
    "$E(\\bar{X}) = \\text{the expected value of $\\bar{X}$}$  \n",
    "$\\mu = \\text{the mean of the population from which the sample is selected}$\n",
    "\n",
    "You may recall from previous statistical courses that this is called an **unbiased** estimator, as the expected value of a point estimator equals the population parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard deviation of $\\bar{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that whenever\n",
    "\n",
    "- the population is infinite or\n",
    "\n",
    "- the population is finite and the sample size is less than or equal to $5\\%$ of the population size ($\\frac{n}{N} \\leq 0.05)$\n",
    "\n",
    "the standard deviation of $\\bar{X}$ is given by:\n",
    "\n",
    "$$\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$\\sigma_{\\bar{x}} = \\text{the standard deviation of $\\bar{X}$}$  \n",
    "$\\sigma = \\text{the standard deviation of the population}$  \n",
    "$n=\\text{the sample size}$  \n",
    "\n",
    "\n",
    "To avoid confusion and to make the distinction between standard deviation of the population / sample and the standard deviation of a point estimate we call the latter **standard error**.\n",
    "\n",
    "For the remainder of this course we assume that the population size is large and that the above condition holds.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **standard error** for our example is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_population = np.std(data[:,0])\n",
    "se = std_population/np.sqrt(smpl_size)\n",
    "se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is fairly close to the standard devision of the sampled means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Form of the sampling distribution of $\\bar{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following two cases:\n",
    "\n",
    "1. Population data is normally distributed\n",
    "\n",
    "2. Population data is not normally distributed\n",
    "\n",
    "\n",
    "In 1. the sampling distribution will also have a normal distribution.\n",
    "\n",
    "In 2. the **central limit theorem** applies, i.e. with large n the sampling distribution for $\\bar{X}$ also can be approximated with a normal distribution.\n",
    "\n",
    " \n",
    "> **Recall: Central limit theorem**  \n",
    "> when selecting random sample of size $n$ from a population, the sampling distribution of the sample mean $\\bar{X}$ is approximated by a normal distribution as the sample size becomes large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a few examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, n = 10_000, 100 # size population, size sample\n",
    "n_smpls = 500 # number of samples taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_dist = stats.binom(3,0.2).rvs(N)#stats.binom(2,2/6).rvs(N)\n",
    "plot_density(pop_dist);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dist(dist,n, n_smpls):\n",
    "    smpl = np.random.choice(dist, size=(n_smpls, n))\n",
    "    return np.mean(smpl, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_dist = sample_dist(pop_dist, n, n_smpls)\n",
    "plot_density(smpl_dist);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we care?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We care about the sample distribution and its properties because it provides useful information about the **difference between the sample mean and the population mean**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** let us assume that we take the sample mean online-spent as an approximation of the population mean online-spent. However, obviously we are only accepting this approximation if we it likely that the sampled mean is close to the true mean. \n",
    "\n",
    "If we are willing to accept a deviation of $1.000$ from the true mean, what is the probability that the sample mean lies within a range of $1.000$ of the population mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can answer this question using our sampling distributions of $\\bar{X}$ by calculating a z-score (i.e. standard normalizing our sampled values) and then use the CDF of the standard normal distribution to calculate probabilities:\n",
    "\n",
    "$$z = \\frac{(x \\pm \\bar{x})}{\\sigma_{\\bar{x}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbar = np.mean(data[:,0])\n",
    "x = xbar - 1000 # lower bound; standard normal distribution is symetrically\n",
    "zscore = (x - xbar) / se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_low = stats.norm(0,1).cdf(zscore)\n",
    "p_up = stats.norm(0,1).cdf(-zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_up - p_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the sample mean - given our sample size - has only a probability of $\\approx 50\\%$ to be within a range of $\\pm 1.000$ from the population mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can increase this probability by increaing the sample size given the **standard error** is a function of sample size $n$.\n",
    "\n",
    "Let's calculate standard error and $z$ for a sample size of 300 instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_size_new = 300\n",
    "std_population = np.std(data[:,0])\n",
    "se = std_population/np.sqrt(smpl_size_new)\n",
    "se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore = (x - xbar) / se\n",
    "zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_low = stats.norm(0,1).cdf(zscore)\n",
    "p_up = stats.norm(0,1).cdf(-zscore)\n",
    "p_up - p_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the sample size to 300 we increasd the probability of the sample mean being within a range of $\\pm 1.000$ of the population mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** analysis of $p$ will be left as an (homework) excercise for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interval estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given point estimators cannot be expected to be an exact value of the true population parameter we usually determine **interval estimates**. We do so by substracting and adding **margin of error** from the point estimator. \n",
    "\n",
    "Let's look into more detail how this is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population standard deviation $\\sigma$ is known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this is usually not the case let's assume that we can reasonably derive knowledge about the population standard deviation ($\\sigma$) from historical data. \n",
    "\n",
    "How can we then construct interval estimates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interval estimate of a population mean (when $\\sigma$ is known):**\n",
    "\n",
    "$$\\bar{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$(1-\\alpha) = \\text{the confidence coefficient or level}$\n",
    "$z_{\\alpha/2} = \\text{the z value providing an area $\\alpha/2$ in the upper tail of the standard normal probability distribution}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: we take a customer survey with $80$ customers to determine customer satisfaction. \n",
    "\n",
    "(Let's assume that customer satisfaction follows a normal distribution with $\\mu=80$ and $\\sigma = 12$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate population data (for demonstration purposes)\n",
    "mu, sig = 80, 12\n",
    "customers = stats.norm(mu,sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample and calculate the sample mean xbar\n",
    "n = 80\n",
    "smpl = customers.rvs(n)\n",
    "xbar = np.mean(smpl)#\n",
    "xbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our many previous customer surveys we \"know\" (i.e. we assume) that the standard deviation of customer satisfaction is 12. \n",
    "\n",
    "We want to be 95% sure that the interval estimate contains the true population mean (i.e. the true customer satisfaction score). This means that $\\alpha = 5\\%$ and we need to determine the $z_{\\alpha/2}$, i.e. the value of the standard normal distribution where only $2.5\\%$ ($\\alpha/2$) are above.\n",
    "\n",
    "We are looking for the value $z$ where `stats.norm(0,1).cdf(z)` $\\approx 2.5\\%$. This could be done using trial and error. However, it is easier to use the `stats.norm.ppf` function which does exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = .95\n",
    "alpha = 1 - ci\n",
    "z = stats.norm(0,1).ppf(alpha/2)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = sig/np.sqrt(n)\n",
    "margin =  z*se\n",
    "interval = xbar +  np.array((margin, -margin))\n",
    "interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this interval we are 95% sure that the interval contains the true population mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population standard deviation $\\sigma$ is unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many real life cases we don't know what the population standard deviation is. If we knew we mostly wouldn't need to calculate estimates in the first place. If $\\sigma$ is unknown we must use the **sample standard deviation ($s$)**. \n",
    "\n",
    "In this case the margin of error and the interval estimate is based on the probability distribution known as the **$t$ distribution**.\n",
    "\n",
    "\n",
    "The t distribution is similar to the normal distribution. A specific t distribution depends on one parameter, the **degree of freedom**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "df = 1 # The more degree of freedom the more t-distribution converges to normal distribution\n",
    "tdist = stats.t(df=df)\n",
    "norm = stats.norm()\n",
    "x = np.linspace(-4,4,N)\n",
    "_, ax = plot_line(x, [tdist.pdf(x), norm.pdf(x)],title=f\"t-Distribution (df = {df}) \\nvs.\\n Standard normal distribution\", zero_origin=False);\n",
    "ax.legend([\"t-Distribution\", \"Standard Normal Distribution\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With higher degrees of freedom the t-distribution can be approximated by the normal distribtution. Which can be shown when calculating the t and z values repectively. As a rule of thumb we can assume that with a degree of freedom of $\\geq 100$ the standard normal z value provides a good approximation to the t value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 100\n",
    "alpha = 0.025\n",
    "t = stats.t(df=df)\n",
    "t.ppf(alpha), stats.norm().ppf(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dist(a=0.25, dist=stats.norm(), ax=None):\n",
    "    # Helper function to show a distribution\n",
    "    low, high = -4, 4\n",
    "    x = np.linspace(low,high,1000)\n",
    "    y = dist.pdf(x)\n",
    "    tvalue = t.ppf(0.025)\n",
    "    xlow = np.linspace(low, tvalue)\n",
    "    xhigh = np.linspace(-tvalue,high)\n",
    "    if ax==None: fig, ax = plt.subplots(figsize=(9,7))\n",
    "    ax.plot(x,y)\n",
    "    ax.fill_between(xlow,dist.pdf(xlow), color=\"red\", alpha=0.4)\n",
    "    ax.fill_between(xhigh,dist.pdf(xhigh), color=\"red\", alpha=0.4)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_position(\"zero\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 1\n",
    "ax = show_dist();\n",
    "show_dist(dist=stats.t(df),ax=ax); # \n",
    "ax.legend([\"Standard normal distribution\", \"t distribution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we calculate interval estimates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interval estimate of a population mean (when $\\sigma$ is unknown):**\n",
    "\n",
    "$$\\bar{x} \\pm t_{\\alpha/2}\\frac{s}{\\sqrt{n}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$s = \\text{the sample standard deviation}$  \n",
    "$(1-\\alpha) = \\text{the confidence coefficient or level}$  \n",
    "$t_{\\alpha/2} = \\text{the t value providing an area $\\alpha/2$ in the upper tail of the (student's) t probability distribution}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "We have conducted a study with $70$ University sutdents to estimate the mean credit card debt for a population of University students. Given we don't know anything about the population standard deviation we have to work with our sample.  \n",
    "\n",
    "- we compute mean ($\\bar{x}$) and standard deviation ($s$) of the sample ($n = 70$)  \n",
    "- we then compute the margin of error and the interval estimates to determine a range of values that include the true (population) mean with a certainty of $90\\%$\n",
    "\n",
    "(Note: the sample data is fake data created using np.random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl = Datasets.credit_card_debt()\n",
    "xbar, s = np.mean(smpl), np.std(smpl,ddof=1)\n",
    "xbar, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .1\n",
    "n = len(smpl)\n",
    "df = n - 1\n",
    "t = stats.t(df).ppf(alpha/2)\n",
    "error_margin = t*s/np.sqrt(n)\n",
    "ci = xbar + np.array((error_margin, -error_margin))\n",
    "ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a probability of $90\\%$ the true population mean of University student's credit card debt lies within the above range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for a rather small sample size of $n = 70$ the difference between using the standard normal distribution and the t-distribution makes a difference. The interval estimate using the standard normal distribution is tighter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci(data, alpha=0.05, dist=\"norm\"):\n",
    "    '''\n",
    "    Calculate confidence interval for sample using standard normal distribution\n",
    "    '''\n",
    "    n = len(data)\n",
    "    s = np.std(data, ddof=1)\n",
    "    \n",
    "    if dist == \"norm\": \n",
    "        z = stats.norm(0,1).ppf(alpha/2)\n",
    "    elif dist == \"t\": \n",
    "        z = stats.t(len(data)-1).ppf(alpha/2)\n",
    "    else:\n",
    "        raise AttributeError(\"Dist must be either 'norm' or 't'\")\n",
    "    \n",
    "    error_margin = z*s/np.sqrt(n)\n",
    "    \n",
    "    return np.mean(data) + np.array((error_margin, -error_margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci(smpl, alpha=0.1,dist=\"norm\"), ci(smpl, alpha=0.1, dist=\"t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This difference becomes negigible with a larger data set. Below you can see the difference using a sample size of $n = 500$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl = Datasets.credit_card_debt(n=500)\n",
    "ci(smpl, alpha=0.1, dist=\"norm\"), ci(smpl, alpha=0.1, dist=\"t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been using probabilities, theory and assumptions about distributions of populations and samples to calculate estimation intervals. \n",
    "\n",
    "It turns out there is another (for many people more intuitive way) of calculating the above estimation intervals: **bootstrapping**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition for bootstrapping algorithm(s):**\n",
    "\n",
    "In theory we could see how much our sample estimator deviates from the true population parameter if we were able to draw many different sample from the population. We could then see how the sample statistic fluctuates. \n",
    "\n",
    "Bootstraping is build on the assumption that the sample should be a good approximation for the population (otherwise the sampling should be reconsidered because it is e.g. not representative for the population). \n",
    "\n",
    "Therefore, it is reasonable to create new samples by drawing from our sample. \n",
    "\n",
    "In doing so we can see how our estimate changes with different samples and draw conclusions from this. It turns out that this simple approach works surpringly well in many contexts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm for Nonparametric bootstrap:**\n",
    "\n",
    "Given your sample data ${\\{z_i\\}_{i=1}^n}$\n",
    "\n",
    "For $b = 1 \\cdots B$:\n",
    "\n",
    "- resample *with replacement* $n$ observations ${\\{z_i^{b}\\}_{i=1}^n}$\n",
    "\n",
    "- calculate your estimate ${\\hat{\\theta}_b}$, e.g. the mean, using the resampled set of data\n",
    "\n",
    "then ${\\{\\hat{\\theta}_b\\}_{b=1}^B}$ is  an approximation for the sampling distribution for $\\hat{\\theta}$. \n",
    "\n",
    "Given the sample distribution we can then calculate the standard error (by calculating the standard deviation of the sample) and hence can calculate the estimate intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our (fake) credit debt example again.  Let's say we have a sample of $n=500$ students and we want to estimate the mean credit debt of (unknown student) population. We want to make sure that our interval estimate with a $90\\%$ probabiltiy contains the population mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traditional approach from previous chapter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "smpl_original = Datasets.credit_card_debt(n)\n",
    "alpha = 0.1\n",
    "ci(smpl_original,alpha, dist=\"norm\" ) # Note we use standard normal distribution given the large sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bootstrapp approach:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(smpl, sims): \n",
    "    size_smpl = len(smpl) # size for samples drawn from original sample\n",
    "\n",
    "    estimates = np.empty(sims)\n",
    "    for i in range(sims):\n",
    "        new_smpl = np.random.choice(smpl, size=size_smpl, replace=True) # Draw new sample form orginal sample\n",
    "        theta = np.mean(new_smpl) # calculate estimate we want (here: mean)\n",
    "        estimates[i] = theta # append to list of estimates (to save distribution)\n",
    "    \n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sims = 10_000\n",
    "estimates = bootstrap(smpl_original, n_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density(estimates, title=\"Sample distribution for mean estimate\", xlabel=\"Mean credit debt\", ylabel=\"density\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this distribution to calculate statistics, e.g. the standard error of our estimate by calculating the standard deviation of estimates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbar = np.mean(estimates) # mean of our estimates (i.e. our point estimate of the true parameter)\n",
    "se = np.std(estimates) # standard error of our point estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = stats.norm().ppf(alpha/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbar + np.array((z,-z))*se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the bootstrap algorithm is an extremely powerfull tool to estimate uncertainties associated with an estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determining estimation intervals for proportions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our very first example we have used online spent data and calculated two estimators:\n",
    "\n",
    "1. mean of online spent\n",
    "\n",
    "2. proportion of people having children. \n",
    "\n",
    "We then looked at 1. to show how to calculate interval estimates; in particular we needed to calculate the standard error for the mean to do this. \n",
    "\n",
    "In order to calculate the standard error for the proportion, you may recall from undergrad statistics, we need to apply a different formula (because we are looking to calculate the standard deviation from a binomial distributed variable). \n",
    "\n",
    "the standard deviation of $p$ is given by:\n",
    "\n",
    "$$\\sigma_{p} = \\sqrt{\\frac{(p(1-p)}{n}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$p = \\text{proportion}$   \n",
    "$n = \\text{the sample size}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the bootsrap approch we can calculate interval estimates for the proportion directly without knowledge about the above formular given we approximate the sample distribution and can thus determine the required statistics directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Datasets.fake_spent_children()\n",
    "children = data[:,1]\n",
    "p = np.mean(children)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.mean(children)\n",
    "se = np.sqrt((p*(1-p)/n))\n",
    "z = 1.645 # proxy for stats.norm.ppf(0.95)\n",
    "p + z*np.array((-se,se))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sims = 10_000\n",
    "estimators = bootstrap(children,n_sims )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = np.std(estimators)\n",
    "low, high = p -1.645*np.array((se,-se))\n",
    "low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plot_density(estimators, title=\"Sample distribution for proportion of households with children ($p$)\\n(confidence limits determined using bootstrap approach)\");\n",
    "ax.vlines(low, 0, 20, linestyle=\"--\")\n",
    "ax.vlines(high, 0, 20, linestyle=\"--\")\n",
    "ax.annotate(f\"Lower limit {low:.4f}\",(low, 22),ha=\"right\");\n",
    "ax.annotate(f\"Upper limit {high:.4f}\",(high, 22),ha=\"left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine confidence interval for linear regression coefficient**\n",
    "\n",
    "We can use the bootstrap algorithm for model parameters such as regression coefficients as well. This shows the flexibility and power of using bootstrap analysis. \n",
    "\n",
    "We will use **advertising** data from [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/data.html) for our next example. \n",
    "\n",
    "For our example the data contains\n",
    "\n",
    "- data on TV advertising spent\n",
    "\n",
    "- sales data\n",
    "\n",
    "We run a simle (linear) regression model of the following type:\n",
    "\n",
    "$$\\hat{y} \\approx \\beta_0 + \\beta_1x$$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\hat{y} = $ sales data  \n",
    "$x = $ tv advertising spent\n",
    "\n",
    "We want to understand if there is a relation between money spent on tv advertising and generated sales. If not, we should stop or alter tv advertisment. In order to determine the relation we estimate $\\beta_1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Datasets.advertising() \n",
    "tv, sales = data[:,0], data[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily calculate coefficients and additional output for regression analysis using the below \"helper\" function which uses special statistical packages for these purposes (we will introduce them in next lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import lreg_summary\n",
    "\n",
    "lreg_summary(tv,sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that $\\beta_1$ (denoted $x_1$) is $0.0475$ with a interval estimates of $0.042$ to $0.053$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try to recalculate the confidence intervals for $x1$ using a bootstrap approach.**\n",
    "\n",
    "Our estimator in this case is not the mean or a proportion but $\\beta_1$ which - you may recal from undergraduate statistics - can be calculated as follows:\n",
    "\n",
    "$$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to do the following:\n",
    "\n",
    "1. implement a function that estimates $\\beta_1$\n",
    "\n",
    "2. bootstrap $beta_1$ repeatedly to generate a distribution of sampled estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create function for beta1\n",
    "def beta1(X,Y):\n",
    "    return np.sum((X - np.mean(X))*(Y-np.mean(Y))) / np.sum((X - np.mean(X))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. bootstrap\n",
    "n = len(data)\n",
    "sims = 5000\n",
    "betas = np.empty(sims)\n",
    "for i in range(sims):\n",
    "    rows = np.random.choice(range(n),n)\n",
    "    X,Y = data[rows,0], data[rows,1]\n",
    "    betas[i] = beta1(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density(betas);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence interval from sample distribution\n",
    "alpha = 0.05\n",
    "z = stats.norm.ppf(alpha/2) # alpha is 0.05\n",
    "np.mean(betas) + np.array((z,-z))*np.std(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to confirm the confidence intervals (although you may not recall how to actually calculate them) by using a bootstrap approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While for our examples most statistical package have \"out of the box\" functionality, this approach gives us confidence to calculate uncertainty ranges even when using complex models or estimators where no out of the box functionality might be available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantage**\n",
    "\n",
    "- does not make strong assumptions about population or sample distributions\n",
    "\n",
    "- often theoretical sampling distributions are not normally distributed\n",
    "\n",
    "- is to understand without knowledge in probability theory / about probability distributions\n",
    "\n",
    "- no knowledge about formulas for standard errors required \n",
    "\n",
    "**Disadvantage**\n",
    "\n",
    "- not feasible for large samples because of computational effort \n",
    "\n",
    "- computational effort when sometimes a simple formular yields an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to Imai (2017), p. 342 to 362 for background information on hypothesis testing. It is generally assumed that you are familiar with hypothesis testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples using python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some examples of how to calculate simple t-tests. We will not cover this in detail as these standard statistics are calculated by statistical packages we will be using when calculating more difficult models. \n",
    ":\n",
    "To get an impression how to conduct such tests in python see below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two-tailed test (one sample)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: It is generally assumed that stock returns on average have a daily return of $0$. \n",
    "\n",
    "Let's build our hypothesis:\n",
    "\n",
    "\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "& H_0: \\mu = 0 \\\\\n",
    "& H_1: \\mu \\neq 0 \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "\n",
    "$\\mu = $ daily stock return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this in numpy from scratch. Recall that the test statistic is given by:\n",
    "\n",
    "$$z = \\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dax 30 daily data\n",
    "dax = Datasets.dax_daily() # prices\n",
    "ret = dax[1:] / dax[:-1] - 1 # calculate returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0 # our assumed population mean of daily returns\n",
    "xbar = np.mean(ret)\n",
    "n = len(ret)\n",
    "se = np.std(ret,ddof=1) / np.sqrt(n)\n",
    "\n",
    "\n",
    "z = (xbar - mu) / se\n",
    "pvalue = (1-stats.t(len(ret) -1).cdf(t))*2 # multiply by two because it is a two-sided t-test\n",
    "z, pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be done using `scipy.stats.ttest_..`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 3\n",
    "n = 10\n",
    "s / np.sqrt(n) == np.sqrt(s**2 / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvalue, pvalue = stats.ttest_1samp(ret, mu) # t-statistic and pvalue\n",
    "tvalue, pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two-Sample Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: let's consider a medical trial where we have to groups the trial group (receiving a new medicine) and a control group (receiving a placebo). We now want to understand of the two groups differ regarding a specific parameter. \n",
    "\n",
    "Let's build our hypothesis:\n",
    "\n",
    "\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "& H_0: \\mu_0 = \\mu_1 \\\\\n",
    "& H_1: \\mu_0 \\neq \\mu_1 \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "\n",
    "$\\mu = $ is e.g. the mean of a specific medical parameter (e.g. blood sugar). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement this in python. Let's create two groups with random data both $\\sim$ $N(100,10)$. A two-sample test should yield that both groups do not differ this is $H_0$ cannot be rejected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_group = stats.norm(100,10).rvs(1000)\n",
    "control_group = stats.norm(100,10).rvs(1000)\n",
    "stats.ttest_ind(trial_group, control_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is indeed the case given the pvalue of $69\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caution with p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2016 an article in *The American Statistician* ([here](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)) criticized the \n",
    "\n",
    "\"[...] widespread use of “statistical significance” (generally interpreted as “p 0.05”) as a license for making a claim of a scientific finding (or implied truth) leads to considerable distortion of the scientific process.\"\n",
    "\n",
    "Unfortunately, this practice is still quite common in research. It should be noted that research does not mean \"finding effects with a p-value below 5%\". While there is nothing wrong or false about the concept of p-values it is sometimes misused. \n",
    "\n",
    "This is due to several reasons:\n",
    "\n",
    "a. p value tends to decrease with largers samples, i.e. the larger the sample the higher the probability of finding a \"significant effect\" (ceteris paribus)\n",
    "\n",
    "b. focus is too much on identifying signficiance while confirming the null hypothesis should actually be considered as a finding as well. Instead behaviors termed \"p-hacking\" becomes common ([see here](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.590.4360&rep=rep1&type=pdf))\n",
    "\n",
    "c. focus on one value is misleading; instead at least ranges should be reported; at best additional tests and indication for confirmation of research hypotheses should be provided. \n",
    "\n",
    "d. statistical significance is often confused with practical relevance \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why p value may be misleading given a large sample size**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy of p value hacking is dangerous and may lead to wrong research findings. \n",
    "\n",
    "In the following we will see that larger samples may lead to statistical effects although the practical relevance of theses statistical significant findings are practically irrelevant. \n",
    "\n",
    "Let's consider the following example (from Daniela Witten): \n",
    "\n",
    "- we are testing whether a mean of a random varialbe $X \\sim N(0,1)$ is equal to $0$\n",
    "\n",
    "- we draw a sample and find that the mean ($\\bar{x}$) is slightly different from 0 (e.g. 0.000001)\n",
    "\n",
    "- Practically we can conclude for (almost all) practical research hypothesis that $H_0$ can be confirmed and that the mean of a sample is $0$. \n",
    "\n",
    "- However, depending on the size of your sample correctly calculated p-value of your estimator will actually indicate rejection of said hypothesis. \n",
    "\n",
    "Let's look at an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a large sample size we can use the normal distribution and the z-value to test for significance. \n",
    "\n",
    "Recall that the test statistic is given by:\n",
    "\n",
    "$$z = \\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}}$$\n",
    "\n",
    "As per our assumption \n",
    "\n",
    "- $s$ will be 1\n",
    "\n",
    "- $\\mu_0$ will be zero\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$$z = \\bar{x}*\\sqrt{n} = 0.000001 * \\sqrt{n}$$\n",
    "\n",
    "If the sample size is big (e.g. $10^6$) - which in the age of big data cannot be considered an exception anymore - the z value will become very big (yielding a low p-value) and leads to the conclusion that $H_0$ needs to be rejected. \n",
    "\n",
    "However, for all practical reasons $\\bar{x}$ is very close to zero and we only observe this finding given the large sample size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10e6\n",
    "xbar = 0.000001\n",
    "zvalue = n*xbar\n",
    "zvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue = 1 - stats.norm(0,1).cdf(zvalue)\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not mean that p-values are wrong. Statistically speaking the result is correct. However, it should be noted that with large sample size we are to see statistical effects that may in fact no indicate practical findings given the deviance from $H_0$ is negligible.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}